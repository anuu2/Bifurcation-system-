# -*- coding: utf-8 -*-
"""Mail_Bifurcation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T6OiaQ9gT6UKpAr2yNvj7UUhr04d3Sxa
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import re
import string
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from pandas import get_dummies
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense,Masking
import openpyxl

path = 'Data_of_mails.xlsx'
load = pd.read_excel(path)

# """# ***Data Processing***

# """

load = load.drop(load.index[[2019,2020]])

initial_data = load['Emails'].tolist()
# #print(initial_data)

data = [[i]for i in initial_data ]
# #print(data)
# #print(len(data))


data_filter = []
issue_indices = []
j=0
for x in data:
  if type(x[0])!=str:
    issue_indices.append(i)
  else:
    data_filter.append(x)
  j=j+1

# #len(data_filter)
# #print(issue_indices)

lower_data = [[i[0].lower()] for i in data_filter]
# #print(lower_data)

text = ' '.join(i[0] for i in lower_data)
# #print(text)


# # Download stopwords
nltk.download('stopwords')
nltk.download('punkt')

global removals 

removals = f"[{re.escape(string.punctuation)}]"
text_processed = re.sub(removals, ' ', text)

# text_processed

words = word_tokenize(text_processed)
# #words
# #len(words)

filter_text = " ".join(words)
# filter_text

global stop_words
stop_words = set(stopwords.words('english'))

filter_list = [x for x in words if x not in stop_words]
# filter_list

final_text = " ".join(filter_list)
# final_text

token = Tokenizer()
token.fit_on_texts([final_text])
# #print(token.word_index)

processed_list = []
for i in lower_data:
  mail = re.sub(removals, ' ', i[0])
  mail_token = word_tokenize(mail)
  filter_mail_list = [x for x in mail_token if x not in stop_words]
  final_mail = " ".join(filter_mail_list)
  processed_list.append(final_mail)

# #processed_list[0]

tokened_mails = token.texts_to_sequences(processed_list)

# #print(tokened_mails)

# """# ***Target Feature Processing***"""

target = load['Type']
target_list = target.tolist()
# #len(target_list)

# """# **Padding**"""

lengths = [len(i) for i in tokened_mails]
#print(lengths)
maxsize =  max(lengths)
# #print(maxsize)

padded_list = pad_sequences(tokened_mails, maxlen=maxsize, padding='post')
# #print(padded_list)

target_list1 = target_list
# #print(target_list1)

num_classes = len(set(target_list1))
# #num_classes



final_target_list = get_dummies(target_list1)
# #print(final_target_list)
# #print(type(final_target_list))


def getVariableByValue(input_value):
    dictionary = ["Claim",  "Complaint", "Inquiry",  "Query",  "Renewal"]
    return dictionary[input_value]

# '''
# """# ***Data Splitting***"""

# X_train, X_temp, Y_train, Y_temp = train_test_split(padded_list, final_target_list, test_size=0.35, random_state=42)
# X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)

# print(X_train)
# print(X_train.shape)
# type(X_train)

# print(Y_train)
# print(Y_train.shape)

# print(X_val)
# print(X_val.shape)

# print(Y_val)
# print(Y_val.shape)

# print(X_test)
# print(X_test.shape)

# print(Y_test)
# print(Y_test.shape)

# all_index = Y_test.index.tolist()
# print(all_index)

# Y_test_orignal = [target_list1[i] for i in all_index]
# Y_test_orignal

# """# ***Model***"""



# model1 = Sequential()

# #model.add(Masking(mask_value=0.0, input_shape=(None,maxsize)))
# model1.add(Embedding(input_dim=len(token.word_index)+1, output_dim=200, input_length=maxsize,mask_zero=True))
# model1.add(LSTM(units=200))
# model1.add(Dense(units=5, activation='softmax'))

# model1.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# model1.summary()

# model1.fit(X_train,Y_train,validation_data =(X_val,Y_val),batch_size=32,epochs=50)

# model1.save("Email_Bifuraction_model.h5")


# """# **Testing and visualising **"""

# Y_pred = model1.predict(X_test)

# print(Y_pred)



# Prediction = [getVariableByValue(list(Y_pred[i]).index(max(Y_pred[i]))) for i in range(len(Y_pred))]
# Prediction

# from sklearn.metrics import accuracy_score
# accuracy = accuracy_score(Y_test_orignal, Prediction)
# print("Accuracy:", accuracy)

# from sklearn.metrics import confusion_matrix
# import seaborn as sns
# import matplotlib.pyplot as plt

# cm = confusion_matrix(Y_test_orignal, Prediction)
# sns.heatmap(cm, annot=True, cmap="YlOrRd")
# plt.xlabel("Predicted Labels")
# plt.ylabel("True Labels")
# plt.title("Confusion Matrix")
# plt.show()
# '''
# """# ***Model Loading and pridicting for any custom mail***"""

def GetPredictionByModel(MAIL):
  saved_model = load_model("1st_model.h5")
  print("lenght 2 : ",len(MAIL))
  predict_proccessed =  re.sub(removals, ' ',MAIL)
  #predict_proccessed

  text_tokenized = word_tokenize(predict_proccessed)
  #text_tokenized

  filter_test_txt = [x for x in text_tokenized if x not in stop_words]
  final_test_txt = ' '.join(x for x in filter_test_txt)
  #final_test_txt

  tokened_final = token.texts_to_sequences([final_test_txt])
  #tokened_final

  padded_final = pad_sequences(tokened_final, maxlen=maxsize, padding='post')
  #padded_final

  pred = saved_model.predict(padded_final)
  #print(pred)
  result = getVariableByValue(list(pred[0]).index(max(pred[0])))
  #print(result)
  return result


# text_test = "Query regarding the renewal of my insurance policy - I hope that this email finds you well. My insura Thank you!!.he new policies and their T&C's in order to renew my policy"
# print(GetPredictionByModel(text_test))